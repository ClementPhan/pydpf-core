{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compute total displacement from distributed files with distributed post {#ref_distributed_delegate_to_server}\r\n\r\nThis example shows how distributed files can be read and post processed\r\non distributed processes. After remote post processing of total\r\ndisplacement, results a merged on the local process. In this example,\r\nthe client is only connected to the coordinator server. Connections to\r\nremote proceses are only done implicitly through the coordinator.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import dpf module and its examples files\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ansys.dpf import core as dpf\nfrom ansys.dpf.core import examples\nfrom ansys.dpf.core import operators as ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the template workflow of total displacement\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "template_workflow = dpf.Workflow()\ndisplacement = ops.result.displacement()\nnorm = ops.math.norm_fc(displacement)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add the operators to the template workflow and name its inputs and\r\noutputs Once workflow\\'s inputs and outputs are named, they can be\r\nconnected later on\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "template_workflow.add_operators([displacement, norm])\ntemplate_workflow.set_input_name(\"data_sources\", displacement.inputs.data_sources)\ntemplate_workflow.set_output_name(\"out\", norm.outputs.fields_container)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure the servers\r\n\r\nMake a list of ip addresses an port numbers on which dpf servers are\r\nstarted. Workflows instances will be created on each of those servers to\r\naddress each a different result file. In this example, we will post\r\nprocess an analysis distributed in 2 files, we will consequently require\r\n2 remote processes To make this example easier, we will start local\r\nservers here, but we could get connected to any existing servers on the\r\nnetwork. We only keep instances of remote_servers to start and keep\r\nthose servers awaik. The purpose of this example is to show that we can\r\ndo distributed post processing without opening channels between this\r\nclient and the remote processes\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "remote_servers = [dpf.start_local_server(as_global=False), dpf.start_local_server(as_global=False)]\nips = [remote_server.ip for remote_server in remote_servers]\nports = [remote_server.port for remote_server in remote_servers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the ips and ports\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"ips:\", ips)\nprint(\"ports:\", ports)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we show how we could send files in temporary directory if we were\r\nnot in shared memory\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "files = examples.download_distributed_files()\nserver_file_paths = [dpf.upload_file_in_tmp_folder(files[0], server=remote_servers[0]),\n                     dpf.upload_file_in_tmp_folder(files[1], server=remote_servers[1])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Send workflows on servers\r\n\r\nHere we create new instances on the server by copies of the template\r\nworkflow We also connect the data sources to those workflows\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "remote_workflows = []\nfor i, ip in enumerate(ips):\n    remote_workflows.append(template_workflow.create_on_other_server(ip=ip, port=ports[i]))\n    ds = dpf.DataSources(server_file_paths[i])\n    remote_workflows[i].connect(\"data_sources\", ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create a local workflow able to merge the results\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "local_workflow = dpf.Workflow()\nmerge = ops.utility.merge_fields_containers()\nlocal_workflow.add_operator(merge)\nlocal_workflow.set_input_name(\"in0\", merge, 0)\nlocal_workflow.set_input_name(\"in1\", merge, 1)\nlocal_workflow.set_output_name(\"merged\", merge.outputs.merged_fields_container)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Connect the workflows together and get the output\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, ip in enumerate(ips):\n    local_workflow.connect_with(remote_workflows[i], (\"out\", \"in\" + str(i)))\n\nfc = local_workflow.get_output(\"merged\", dpf.types.fields_container)\nprint(fc)\nprint(fc[0].min().data)\nprint(fc[0].max().data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shutdown the servers\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dpf.server.shutdown_all_session_servers()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}