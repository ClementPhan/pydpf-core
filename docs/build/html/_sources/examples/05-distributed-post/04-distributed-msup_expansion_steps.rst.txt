
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples\05-distributed-post\04-distributed-msup_expansion_steps.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_05-distributed-post_04-distributed-msup_expansion_steps.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_05-distributed-post_04-distributed-msup_expansion_steps.py:


.. _ref_distributed_msup_steps:

Distributed msup distributed modal response
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This example shows how distributed files can be read and expanded
on distributed processes. The modal basis (2 distributed files) is read 
on 2 remote servers and the modal response (2 distributed files) reading and the expansion is 
done on a third server.

.. GENERATED FROM PYTHON SOURCE LINES 14-15

Import dpf module and its examples files

.. GENERATED FROM PYTHON SOURCE LINES 15-20

.. code-block:: default


    from ansys.dpf import core as dpf
    from ansys.dpf.core import examples
    from ansys.dpf.core import operators as ops








.. GENERATED FROM PYTHON SOURCE LINES 21-24

Create the template workflow 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
this workflow will provide the modal basis and the mesh for each domain

.. GENERATED FROM PYTHON SOURCE LINES 24-29

.. code-block:: default


    template_workflow = dpf.Workflow()
    displacement = ops.result.displacement()
    mesh = ops.mesh.mesh_provider()








.. GENERATED FROM PYTHON SOURCE LINES 30-32

Add the operators to the template workflow and name its inputs and outputs
Once workflow's inputs and outputs are named, they can be connected later on

.. GENERATED FROM PYTHON SOURCE LINES 32-38

.. code-block:: default

    template_workflow.add_operators([displacement])
    template_workflow.set_input_name("data_sources", displacement.inputs.data_sources)
    template_workflow.set_input_name("data_sources", mesh.inputs.data_sources)
    template_workflow.set_output_name("out", displacement.outputs.fields_container)
    template_workflow.set_output_name("outmesh", mesh.outputs.mesh)








.. GENERATED FROM PYTHON SOURCE LINES 39-48

Configure the servers
~~~~~~~~~~~~~~~~~~~~~~
Make a list of ip addresses an port numbers on which dpf servers are 
started. Workflows instances will be created on each of those servers to 
address each a different result file.
In this example, we will post process an analysis distributed in 2 files,
we will consequently require 2 remote processes
To make this example easier, we will start local servers here, 
but we could get connected to any existing servers on the network.

.. GENERATED FROM PYTHON SOURCE LINES 48-53

.. code-block:: default


    remote_servers = [dpf.start_local_server(as_global=False), dpf.start_local_server(as_global=False)]
    ips = [remote_server.ip for remote_server in remote_servers]
    ports = [remote_server.port for remote_server in remote_servers]



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "D:\AnsysDev\dpf-python-core\examples\05-distributed-post\04-distributed-msup_expansion_steps.py", line 49, in <module>
        remote_servers = [dpf.start_local_server(as_global=False), dpf.start_local_server(as_global=False)]
      File "d:\ansysdev\dpf-python-core\ansys\dpf\core\server.py", line 178, in start_local_server
        server = DpfServer(ansys_path, ip, port,as_global= as_global, load_operators = load_operators)
      File "d:\ansysdev\dpf-python-core\ansys\dpf\core\server.py", line 310, in __init__
        self._session = session.Session(self)
      File "d:\ansysdev\dpf-python-core\ansys\dpf\core\session.py", line 31, in __init__
        if server_meet_version("3.0", self._server):
      File "d:\ansysdev\dpf-python-core\ansys\dpf\core\check_version.py", line 28, in server_meet_version
        version = get_server_version(server)
      File "d:\ansysdev\dpf-python-core\ansys\dpf\core\check_version.py", line 114, in get_server_version
        version = server.version
      File "d:\ansysdev\dpf-python-core\ansys\dpf\core\server.py", line 365, in version
        return self._base_service.server_info["server_version"]
      File "d:\ansysdev\dpf-python-core\ansys\dpf\core\server.py", line 316, in _base_service
        self.__base_service = BaseService(self, timeout=1)
      File "d:\ansysdev\dpf-python-core\ansys\dpf\core\core.py", line 287, in __init__
        self._stub = self._connect(timeout)
      File "d:\ansysdev\dpf-python-core\ansys\dpf\core\core.py", line 301, in _connect
        raise IOError(f'Unable to connect to DPF instance at {self._server()._input_ip} {self._server()._input_port}')
    OSError: Unable to connect to DPF instance at 10.110.2.65 50055




.. GENERATED FROM PYTHON SOURCE LINES 54-55

Print the ips and ports

.. GENERATED FROM PYTHON SOURCE LINES 55-58

.. code-block:: default

    print("ips:", ips)
    print("ports:", ports)


.. GENERATED FROM PYTHON SOURCE LINES 59-60

Choose the file path

.. GENERATED FROM PYTHON SOURCE LINES 60-65

.. code-block:: default


    base_path = examples.distributed_msup_folder
    files = [base_path + r'\file0.mode', base_path + r'\file1.mode']
    files_aux = [base_path + r'\file0.rst', base_path + r'\file1.rst']


.. GENERATED FROM PYTHON SOURCE LINES 66-70

Send workflows on servers
~~~~~~~~~~~~~~~~~~~~~~~~~~
Here we create new instances on the server by copies of the template workflow
We also connect the data sources to those workflows 

.. GENERATED FROM PYTHON SOURCE LINES 70-77

.. code-block:: default

    remote_workflows = []
    for i, server in enumerate(remote_servers):
        remote_workflows.append(template_workflow.create_on_other_server(server))
        ds = dpf.DataSources(files[i])
        ds.add_file_path(files_aux[i])
        remote_workflows[i].connect("data_sources", ds)


.. GENERATED FROM PYTHON SOURCE LINES 78-82

Create a local workflow for expansion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In this workflow we merge the modal basis, the meshes, read the modal response
and expand the modal response with the modal basis

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: default


    local_workflow = dpf.Workflow()
    merge = ops.utility.merge_fields_containers()
    merge_mesh = ops.utility.merge_meshes()

    ds = dpf.DataSources(base_path + r'\file_load_1.rfrq')
    response = ops.result.displacement(data_sources=ds)
    response.inputs.mesh(merge_mesh.outputs.merges_mesh)

    ds = dpf.DataSources(base_path + r'\file_load_2.rfrq')
    response2 = ops.result.displacement(data_sources=ds)
    response2fc = response2.outputs.fields_container()
    response2fc.time_freq_support.time_frequencies.scoping.set_id(0, 2)

    merge_use_pass = ops.utility.merge_fields_containers()
    merge_use_pass.inputs.fields_containers1(response)
    merge_use_pass.inputs.fields_containers2(response2fc)

    expansion = ops.math.modal_superposition(solution_in_modal_space=merge_use_pass, modal_basis=merge)
    component = ops.logic.component_selector_fc(expansion, 1)

    local_workflow.add_operators([merge, merge_use_pass, expansion, merge_mesh, component])
    local_workflow.set_input_name("in0", merge, 0)
    local_workflow.set_input_name("in1", merge, 1)
    local_workflow.set_input_name("inmesh0", merge_mesh, 0)
    local_workflow.set_input_name("inmesh1", merge_mesh, 1)

    local_workflow.set_output_name("expanded", component.outputs.fields_container)
    local_workflow.set_output_name("mesh", merge_mesh.outputs.merges_mesh)


.. GENERATED FROM PYTHON SOURCE LINES 113-115

Connect the workflows together and get the output
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 115-125

.. code-block:: default


    for i, server in enumerate(remote_servers):
        local_workflow.connect_with(remote_workflows[i], {"out": "in" + str(i), "outmesh": "inmesh" + str(i)})

    fc = local_workflow.get_output("expanded", dpf.types.fields_container)
    merged_mesh = local_workflow.get_output("mesh", dpf.types.meshed_region)
    merged_mesh.plot(fc.get_field_by_time_complex_ids(1, 0))
    merged_mesh.plot(fc.get_field_by_time_complex_ids(20, 0))
    print(fc)


.. GENERATED FROM PYTHON SOURCE LINES 126-128

Shutdown the servers
~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 128-128

.. code-block:: default

    dpf.server.shutdown_all_session_servers()

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  3.334 seconds)


.. _sphx_glr_download_examples_05-distributed-post_04-distributed-msup_expansion_steps.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 04-distributed-msup_expansion_steps.py <04-distributed-msup_expansion_steps.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 04-distributed-msup_expansion_steps.ipynb <04-distributed-msup_expansion_steps.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
